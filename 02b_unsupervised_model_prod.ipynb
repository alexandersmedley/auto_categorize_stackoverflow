{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "import joblib\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = r\"\"\"!\"#$%&'()*+,./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "gsp.RE_PUNCT = re.compile(r'([%s])+' % re.escape(punctuation), re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and apply gensim filters\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "#            gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    \n",
    "    s = s.split()\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Custom transformer using gensim filters\n",
    "class TextCleaner(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_topics_opt : 140\n",
    "n_top_words_opt = 25\n",
    "min_proba_opt = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore.load('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary.load('lda_model.id2word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tags = joblib.load('lda_tags.list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a list of tags for a single document in bow format using the lda_model and the tags associated to each topic\n",
    "def lda_tag_pred(lda_model, document, lda_tags, min_proba = 0):\n",
    "    # Get document topics from lda_model\n",
    "    topics_pred = lda_model.get_document_topics(document)\n",
    "    \n",
    "    # Get topic tags for every topic with proba greater than min_proba and combine them in a list\n",
    "    tags_pred = []\n",
    "    for topic_nb, proba in topics_pred:\n",
    "        if proba > min_proba :\n",
    "            tags_pred.extend(lda_tags[topic_nb])\n",
    "    \n",
    "    # Remove potential duplicates\n",
    "    tags_pred = list(set(tags_pred))\n",
    "    \n",
    "    return tags_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prod = ['''How to improve model loss and accuracy? \n",
    "\n",
    "I'm currently using a Unet model taken from the kaggle starter code, and modified a couple of parameters to train it on the TACO Dataset for litter. Right now, I'm at a loss as to how I should proceed with optimizing my model. I'm experiencing ridiculous amounts of loss and abysmal accuracy, and I'm not entirely sure which parameters would improve my model's accuracy and loss. The TACO dataset has 60 categories (61 including background). Am I doing something wrong? I'm pretty new to this so any references I could read or advice would be much appreciated.\n",
    "\n",
    "Here is the code for my model:\n",
    "\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "IMG_CHANNELS = 3\n",
    "epochs = 25\n",
    "validation_steps = val_size\n",
    "steps_per_epoch = train_size\n",
    "\n",
    "##Creating the model\n",
    "\n",
    "initializer = \"he_normal\"\n",
    "\n",
    "###Building U-Net Model\n",
    "\n",
    "##Input Layer\n",
    "inputs = Input((IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS))\n",
    "\n",
    "##Converting inputs to float\n",
    "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "##Contraction\n",
    "c1 = tf.keras.layers.Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(s)\n",
    "c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "c1 = tf.keras.layers.Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c1)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2,2))(c1)\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(p1)\n",
    "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "c2 = tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c2)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2,2))(c2)\n",
    "\n",
    "c3 = tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(p2)\n",
    "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "c3 = tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c3)\n",
    "p3 = tf.keras.layers.MaxPooling2D((2,2))(c3)\n",
    "\n",
    "c4 = tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(p3)\n",
    "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "c4 = tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c4)\n",
    "p4 = tf.keras.layers.MaxPooling2D((2,2))(c4)\n",
    "\n",
    "c5 = tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(p4)\n",
    "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "c5 = tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c5)\n",
    "\n",
    "##Expansion\n",
    "u6 = tf.keras.layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding=\"same\")(c5)\n",
    "u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "c6 = tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(u6)\n",
    "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "c6 = tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c6)\n",
    "\n",
    "u7 = tf.keras.layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding=\"same\")(c6)\n",
    "u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "c7 = tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(u7)\n",
    "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c7)\n",
    "\n",
    "u8 = tf.keras.layers.Conv2DTranspose(32, (2,2), strides=(2,2), padding=\"same\")(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "c8 = tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(u8)\n",
    "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c8)\n",
    "\n",
    "u9 = tf.keras.layers.Conv2DTranspose(16, (2,2), strides=(2,2), padding=\"same\")(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(u9)\n",
    "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=initializer, padding=\"same\")(c9)\n",
    "\n",
    "##Output Layer\n",
    "outputs = tf.keras.layers.Dense(61, activation=\"softmax\")(c9)\n",
    "\n",
    "##Defining Model\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "##Compiling Model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "##Training the model\n",
    "results = model.fit(x = train_gen, \n",
    "                    validation_data = val_gen, \n",
    "                    steps_per_epoch = steps_per_epoch, \n",
    "                    validation_steps = validation_steps, \n",
    "                    epochs = epochs, \n",
    "                    verbose = True)\n",
    "\n",
    "And here is the accuracy and loss from the first epoch:\n",
    "\n",
    "Epoch 1/25\n",
    " 185/1200 [===>..........................] - ETA: 3:30:04 - loss: 388.0077 - accuracy: 9.0721e-04\n",
    "\n",
    "I'm currently using tensorboard, modelcheckpoint, and earlystopping for callbacks, but unfortunately I don't know how these will help with optimizing my model. Would a larger number of neurons per layer work?\n",
    "''']\n",
    "\n",
    "# real tags = python tensorflow machine-learning keras deep-learning\n",
    "# https://stackoverflow.com/questions/61927516/how-to-improve-model-loss-and-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prod = ['''\n",
    "How to interpret Sklearn LDA perplexity score. Why it always increase as number of topics increase?\n",
    "\n",
    "I try to find the optimal number of topics using LDA model of sklearn. To do this I calculate perplexity by referring code on https://gist.github.com/tmylk/b71bf7d3ec2f203bfce2.\n",
    "\n",
    "But when I increase the number of topics, perplexity always increase irrationally. Am I wrong in implementations or just it gives right values?\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "n_samples = 0.7\n",
    "n_features = 1000\n",
    "n_top_words = 20\n",
    "dataset = kickstarter['short_desc'].tolist()\n",
    "data_samples = dataset[:int(len(dataset)*n_samples)]\n",
    "test_samples = dataset[int(len(dataset)*n_samples):]\n",
    "\n",
    "Use tf (raw term count) features for LDA.\n",
    "\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "t0 = time()\n",
    "tf_test = tf_vectorizer.transform(test_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "Calculate Perplexity for (5, 10, 15 ... 100 topics)\n",
    "\n",
    "for i in xrange(5,101,5):\n",
    "    n_topics = i\n",
    "\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d, n_features=%d n_topics=%d \"\n",
    "          % (n_samples, n_features, n_topics))\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    t0 = time()\n",
    "    lda.fit(tf)\n",
    "\n",
    "    train_gamma = lda.transform(tf)\n",
    "    train_perplexity = lda.perplexity(tf, train_gamma)\n",
    "\n",
    "    test_gamma = lda.transform(tf_test)\n",
    "    test_perplexity = lda.perplexity(tf_test, test_gamma)\n",
    "\n",
    "    print('sklearn preplexity: train=%.3f, test=%.3f' %\n",
    "          (train_perplexity, test_perplexity))\n",
    "\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "Results of Perplexity Calculation\n",
    "\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=5 \n",
    "sklearn preplexity: train=9500.437, test=12350.525\n",
    "done in 4.966s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=10 \n",
    "sklearn preplexity: train=341234.228, test=492591.925\n",
    "done in 4.628s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=15 \n",
    "sklearn preplexity: train=11652001.711, test=17886791.159\n",
    "done in 4.337s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=20 \n",
    "sklearn preplexity: train=402465954.270, test=609914097.869\n",
    "done in 4.351s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=25 \n",
    "sklearn preplexity: train=14132355039.630, test=21945586497.205\n",
    "done in 4.438s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=30 \n",
    "sklearn preplexity: train=499209051036.715, test=770208066318.557\n",
    "done in 4.076s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=35 \n",
    "sklearn preplexity: train=16539345584599.268, test=24731601176317.836\n",
    "done in 4.230s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=40 \n",
    "sklearn preplexity: train=586526357904887.250, test=880809950700756.625\n",
    "done in 4.596s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=45 \n",
    "sklearn preplexity: train=20928740385934636.000, test=31065168894315760.000\n",
    "done in 4.563s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=50 \n",
    "sklearn preplexity: train=734804198843926784.000, test=1102284263786783616.000\n",
    "done in 4.790s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=55 \n",
    "sklearn preplexity: train=24747026375445286912.000, test=36634830286916853760.000\n",
    "done in 4.839s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=60 \n",
    "sklearn preplexity: train=879215493067590729728.000, test=1268331920975308783616.000\n",
    "done in 4.827s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=65 \n",
    "sklearn preplexity: train=30267393208097070645248.000, test=43678395923698735382528.000\n",
    "done in 4.705s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=70 \n",
    "sklearn preplexity: train=1091388615092136975532032.000, test=1564111432914603675222016.000\n",
    "done in 4.626s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=75 \n",
    "sklearn preplexity: train=37463573890268863118966784.000, test=51513357456275195169865728.000\n",
    "done in 5.034s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=80 \n",
    "sklearn preplexity: train=1281758440147129243608809472.000, test=1736796133443165299937378304.000\n",
    "done in 5.348s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=85 \n",
    "sklearn preplexity: train=45100838968058242714191265792.000, test=62725627465378386290422054912.000\n",
    "done in 4.987s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=90 \n",
    "sklearn preplexity: train=1555576278144903954081448460288.000, test=2117105172204280105824751190016.000\n",
    "done in 5.032s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=95 \n",
    "sklearn preplexity: train=52806759455785055803020813533184.000, test=70510180325555822379548402515968.000\n",
    "done in 5.284s.\n",
    "Fitting LDA models with tf features, n_samples=0, n_features=1000 n_topics=100 \n",
    "sklearn preplexity: train=1885916623308147578324101753733120.000, test=2505878598724106449894719231098880.000\n",
    "'''    \n",
    "]\n",
    "\n",
    "# python scikit-learn topic-modeling perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm', 'scala', 'sql', 'tensorflow', 'keras', 'pandas']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prod_prep = TextCleaner().transform(X_prod)\n",
    "corpus_prod = id2word.doc2bow(X_prod_prep[0])\n",
    "lda_tag_pred(lda_model, corpus_prod, lda_tags, min_proba = min_proba_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
